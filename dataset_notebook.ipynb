{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8028cc4",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f23e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision ftfy regex tqdm pillow\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "from ddgs import DDGS\n",
    "import os\n",
    "import requests\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import os, shutil, glob\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def collect_url_and_download_images(specie: str, max_pages: int=50, out_folder: str='dataset'):\n",
    "    outdir = f\"{out_folder}/{specie}/raw\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    for query in [specie, f\"dinosaur {specie}\"]:\n",
    "        for i in range(max_pages):\n",
    "            results.extend(DDGS().images(\n",
    "                query=query,\n",
    "                region=\"us-en\",\n",
    "                safesearch=\"off\",\n",
    "                max_results=1000,\n",
    "                page=i))\n",
    "            \n",
    "    unique = {result[\"image\"] for result in results}\n",
    "    for idx, url in enumerate(unique, start=1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            r.raise_for_status()\n",
    "            ext = os.path.splitext(url.split(\"?\")[0])[1] or \".jpg\"\n",
    "            filename = os.path.join(outdir, f\"{idx:04d}{ext}\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {url} -> {e}\")\\\n",
    "                \n",
    "    return outdir\n",
    "\n",
    "def remove_duplicate_images(folder_path: str, similarity_threshold: int = 4):\n",
    "    \"\"\"\n",
    "    Remove duplicate/similar images using perceptual hashing.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing images\n",
    "        similarity_threshold: Lower = more strict (0=identical, 5=default, 10+=lenient)\n",
    "    \"\"\"\n",
    "    # Get all image files\n",
    "    image_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "    image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'))]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found in folder\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Checking {len(image_files)} images for duplicates...\")\n",
    "    \n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                img_hash = imagehash.phash(img)\n",
    "                \n",
    "                # Check if similar image already exists\n",
    "                for existing_hash in hashes:\n",
    "                    if abs(img_hash - existing_hash) <= similarity_threshold:\n",
    "                        duplicates.append(img_path)\n",
    "                        print(f\"Duplicate: {os.path.basename(img_path)}\")\n",
    "                        break\n",
    "                else:\n",
    "                    hashes[img_hash] = img_path\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(img_path)}: {e}\")\n",
    "            print(f\"Deleting {os.path.basename(img_path)}\")\n",
    "            os.remove(img_path)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for duplicate in duplicates:\n",
    "        os.remove(duplicate)\n",
    "    \n",
    "    print(f\"Removed {len(duplicates)} duplicates. {len(image_files) - len(duplicates)} unique images remain.\")\n",
    "    \n",
    "@torch.no_grad()\n",
    "def score_image(img_path, model, preprocess, device, \n",
    "                TXT_IS_DINO, TXT_NOT_DINO, TXT_REAL, TXT_NONREAL,\n",
    "                is_dino_prompts, not_dino_prompts, realistic_prompts, non_realistic_prompts,\n",
    "                verbose=False):\n",
    "    try:\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    img_feat = model.encode_image(image)\n",
    "    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def logits_for(prompts, txt_feats):\n",
    "        # cosine sims\n",
    "        sims = (img_feat @ txt_feats.T)  # [1, num_prompts]\n",
    "        # apply CLIP temperature (sharpens distribution)\n",
    "        scale = model.logit_scale.exp()\n",
    "        return sims * scale\n",
    "\n",
    "    # Get logits for each category\n",
    "    L_is   = logits_for(is_dino_prompts, TXT_IS_DINO)\n",
    "    L_not  = logits_for(not_dino_prompts, TXT_NOT_DINO)\n",
    "    L_real = logits_for(realistic_prompts, TXT_REAL)\n",
    "    L_non  = logits_for(non_realistic_prompts, TXT_NONREAL)\n",
    "\n",
    "    # 1) Dinosaur classification: max positive vs max negative\n",
    "    max_is_dino = L_is.max().item()\n",
    "    max_not_dino = L_not.max().item()\n",
    "    \n",
    "    # Softmax between the two max logits\n",
    "    dino_logits = torch.tensor([max_is_dino, max_not_dino])\n",
    "    dino_probs = torch.softmax(dino_logits, dim=0)\n",
    "    p_is_dino = dino_probs[0].item()\n",
    "    p_not_dino = dino_probs[1].item()\n",
    "\n",
    "    # 2) Realism classification: max positive vs max negative\n",
    "    max_real = L_real.max().item()\n",
    "    max_non_real = L_non.max().item()\n",
    "    \n",
    "    # Softmax between the two max logits\n",
    "    real_logits = torch.tensor([max_real, max_non_real])\n",
    "    real_probs = torch.softmax(real_logits, dim=0)\n",
    "    p_real = real_probs[0].item()\n",
    "    p_non_real = real_probs[1].item()\n",
    "\n",
    "    # margins in [âˆ’1, 1]; 0 means tie, >0 favors positives\n",
    "    is_dino_margin = p_is_dino - p_not_dino\n",
    "    realism_margin = p_real - p_non_real\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ‘‰ {os.path.basename(img_path)}\")\n",
    "        print(\"  [DINO LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(is_dino_prompts, L_is.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_is.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [NOT_DINO LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(not_dino_prompts, L_not.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_not.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [REALISTIC LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(realistic_prompts, L_real.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_real.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [NON_REALISTIC LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(non_realistic_prompts, L_non.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_non.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(f\"  Max dino: {max_is_dino:.3f} vs Max not-dino: {max_not_dino:.3f}\")\n",
    "        print(f\"  Max real: {max_real:.3f} vs Max non-real: {max_non_real:.3f}\")\n",
    "        print(f\"  p_is_dino={p_is_dino:.3f}  p_real={p_real:.3f}\")\n",
    "        print(f\"  is_dino_margin={is_dino_margin:.3f}  realism_margin={realism_margin:.3f}\")\n",
    "\n",
    "    return is_dino_margin, realism_margin\n",
    "\n",
    "def filter_folder(in_dir, out_good, out_bad, specie,\n",
    "                  dino_thr=0.20, realism_thr=0.15, verbose=False):\n",
    "    \n",
    "    os.makedirs(out_good, exist_ok=True)\n",
    "    os.makedirs(out_bad, exist_ok=True)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    # ---------- PROMPTS ----------\n",
    "    # 1) \"Ãˆ un dinosauro?\"\n",
    "    is_dino_prompts = [\n",
    "        \"a realistic illustration of a dinosaur\",\n",
    "        \"a realistic toy dinosaur figure\",\n",
    "        \"a photo of a dinosaur\",\n",
    "        \"a paleoart illustration of a dinosaur\",\n",
    "        f\"a realistic illustration of a {specie}\",\n",
    "        f\"a realistic toy {specie} figure\",\n",
    "        f\"a photo of a {specie}\",\n",
    "        f\"a paleoart illustration of a {specie}\",\n",
    "    ]\n",
    "    not_dino_prompts = [\n",
    "        \"a photo of a modern animal\",\n",
    "        \"a person or human\",\n",
    "        \"a landscape without animals\",\n",
    "        \"a vehicle or building\",\n",
    "        \"a toy\",\n",
    "        \"a cloth\",\n",
    "        \"an abstract image\",\n",
    "        \"a geometric figure\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 2) Realistico vs Cartoon/Toy/Fossile\n",
    "    realistic_prompts = [\n",
    "        \"a realistic illustration of a dinosaur\",\n",
    "        \"a dinosaur fossil skeleton\",\n",
    "        \"a realistic toy dinosaur figure\",\n",
    "        \"a high-quality render of a dinosaur\",\n",
    "        \n",
    "        f\"a realistic illustration of a {specie}\",\n",
    "        f\"a {specie} fossil skeleton\",\n",
    "        f\"a realistic toy {specie} figure\",\n",
    "        f\"a high-quality render of a {specie}\",\n",
    "    ]\n",
    "    non_realistic_prompts = [\n",
    "        \"a cartoon dinosaur for kids\",\n",
    "        \"a peluche toy dinosaur figure\",\n",
    "        \"a pixel art dinosaur\",\n",
    "        \"a simple line drawing of a dinosaur\",\n",
    "        \"a non realistic drawing of a dinosaur\"\n",
    "    ]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text_batch(prompts):\n",
    "        tokens = clip.tokenize(prompts).to(device)\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    TXT_IS_DINO = encode_text_batch(is_dino_prompts)\n",
    "    TXT_NOT_DINO = encode_text_batch(not_dino_prompts)\n",
    "    TXT_REAL = encode_text_batch(realistic_prompts)\n",
    "    TXT_NONREAL = encode_text_batch(non_realistic_prompts)\n",
    "\n",
    "    imgs = []\n",
    "    for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.bmp\"):\n",
    "        imgs += glob.glob(os.path.join(in_dir, ext))\n",
    "    kept = 0; rejected = 0\n",
    "\n",
    "    for p in tqdm(imgs, desc=f\"Filtering {os.path.basename(in_dir)}\"):\n",
    "        res = score_image(p, model, preprocess, device, \n",
    "                        TXT_IS_DINO, TXT_NOT_DINO, TXT_REAL, TXT_NONREAL,\n",
    "                        is_dino_prompts, not_dino_prompts, realistic_prompts, non_realistic_prompts, \n",
    "                        verbose=verbose)\n",
    "        if res is None:\n",
    "            rejected += 1\n",
    "            shutil.copy(p, os.path.join(out_bad, os.path.basename(p)))\n",
    "            continue\n",
    "        is_dino_m, realism_m = res\n",
    "\n",
    "        if (is_dino_m >= dino_thr) and (realism_m >= realism_thr):\n",
    "            kept += 1\n",
    "            shutil.copy(p, os.path.join(out_good, os.path.basename(p)))\n",
    "        else:\n",
    "            rejected += 1\n",
    "            shutil.copy(p, os.path.join(out_bad, os.path.basename(p)))\n",
    "\n",
    "    return kept, rejected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985388f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\n",
    "    \"Ankylosaurus\",\n",
    "    \"Brachiosaurus\",\n",
    "    \"Compsognathus\",\n",
    "    \"Corythosaurus\",\n",
    "    \"Dilophosaurus\",\n",
    "    \"Dimorphodon\",\n",
    "    \"Gallimimus\",\n",
    "    \"Microceratus\",\n",
    "    \"Pachycephalosaurus\",\n",
    "    \"Parasaurolophus\",\n",
    "    \"Spinosaurus\",\n",
    "    \"Stegosaurus\",\n",
    "    \"Triceratops\",\n",
    "    \"Tyrannosaurus\",\n",
    "    \"Velociraptor\"\n",
    "]\n",
    "\n",
    "for specie in species:\n",
    "    print(f\"processing dinosaur {specie}\")\n",
    "    specie_raw_dir = collect_url_and_download_images(specie=specie)\n",
    "    remove_duplicate_images(folder_path=specie_raw_dir)\n",
    "    print(f\"filtering dinosaur {specie}\")\n",
    "    kept, rej = filter_folder(\n",
    "        specie_raw_dir,\n",
    "        out_good=os.path.join(specie_raw_dir, \"clean\"),\n",
    "        out_bad=os.path.join(specie_raw_dir,\"rejected\"),\n",
    "        specie=specie,\n",
    "        dino_thr=0.20,\n",
    "        realism_thr=0.20)\n",
    "    print(f\"Specie: {specie}. Kept {(kept)}, ejected {(rej)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c27ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dinosaurs in species:\n",
    "    print(f'dino: {dinosaurs}')\n",
    "    folder = f'./dataset/{dinosaurs}/raw/clean'\n",
    "    out_folder = f'dataset/to_phone/{dinosaurs}'\n",
    "    #os.makedirs(out_folder)\n",
    "    for file_name in tqdm(os.listdir(folder)):\n",
    "        source = os.path.join(folder, file_name)\n",
    "        destination = os.path.join(out_folder, file_name)\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy(source, destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a1d8c",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ef604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "curated_path = './dataset/hand_curated_datasets'\n",
    "final_dataset = './dataset/dataset'\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "for species_sub_folder in os.listdir(curated_path):\n",
    "    print(species_sub_folder)\n",
    "    species_images = os.listdir(os.path.join(curated_path,species_sub_folder))\n",
    "    species_images = [image for image in species_images if not image.startswith('.')] \n",
    "    indices = random.sample(range(len(species_images)), int(0.15*len(species_images)))\n",
    "    os.makedirs(os.path.join(final_dataset, 'train', species_sub_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(final_dataset, 'test', species_sub_folder), exist_ok=True)\n",
    "    \n",
    "    for idx, image in enumerate(species_images):\n",
    "        path_to_image = os.path.join(curated_path, species_sub_folder, image)\n",
    "        split = 'test' if idx in indices else 'train'\n",
    "        dst_path = os.path.join(final_dataset, split, species_sub_folder, image)\n",
    "        try:\n",
    "            img = Image.open(path_to_image)\n",
    "            png_filename = os.path.splitext(image)[0] + '.png'\n",
    "            dst_path = os.path.join(final_dataset, split, species_sub_folder, png_filename)\n",
    "            img.save(dst_path, 'PNG')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5a6e0",
   "metadata": {},
   "source": [
    "### Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models import ResNet18_Weights, resnet18\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use ImageNet normalization that matches the pretrained weights\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "base_tfms = weights.transforms()\n",
    "size = 256\n",
    "\n",
    "def visualize_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Visualize a PyTorch image tensor\n",
    "    \n",
    "    Args:\n",
    "    - image_tensor: A torch tensor of shape [3, 224, 224] or [1, 3, 224, 224]\n",
    "    \"\"\"\n",
    "    if image_tensor.dim() == 4:\n",
    "        image_tensor = image_tensor.squeeze(0)\n",
    "    \n",
    "    image_np = image_tensor.cpu().numpy()\n",
    "\n",
    "    image_np = np.transpose(image_np, (1, 2, 0))\n",
    "    \n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image_np = image_np * std + mean\n",
    "    \n",
    "    # Clip values to 0-1 range\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def letterbox_to_square(img, size=256, fill=0):\n",
    "    w, h = img.size\n",
    "    scale = size / max(w, h)\n",
    "    new_w, new_h = int(round(w * scale * 0.8)), int(round(h * scale * 0.8))\n",
    "    img = F.resize(img, (new_h, new_w), antialias=True)\n",
    "    \n",
    "    pad_left   = (size - new_w) // 2\n",
    "    pad_right  = size - new_w - pad_left\n",
    "    pad_top    = (size - new_h) // 2\n",
    "    pad_bottom = size - new_h - pad_top\n",
    "    img = F.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=fill)\n",
    "    return img\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    transforms.Lambda(lambda im: letterbox_to_square(im, size=size, fill=0)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),                 # (or 244 if you really want)\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    base_tfms,                                  # ToTensor + Normalize\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    transforms.Lambda(lambda im: letterbox_to_square(im, size=size, fill=0)),\n",
    "    transforms.CenterCrop(224),\n",
    "    base_tfms,\n",
    "])\n",
    "\n",
    "root = \"./dataset/dataset\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=f\"{root}/train\", transform=train_tfms)\n",
    "test_ds  = datasets.ImageFolder(root=f\"{root}/test\",  transform=test_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random samples and show them\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for ax in axes:\n",
    "    img, label = random.choice(test_ds)\n",
    "    # img is a tensor; unnormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    img_disp = img * std[:, None, None] + mean[:, None, None]\n",
    "    img_disp = img_disp.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "    print(img.shape)\n",
    "    ax.imshow(img_disp)\n",
    "    ax.set_title(test_ds.classes[label])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ff9da",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59abc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ciao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model = vit_b_16(weights=weights)\n",
    "\n",
    "model = model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f44c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named,p in model.named_parameters():\n",
    "    print(named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9eacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lora_pytorch import LoRA\n",
    "\n",
    "# Replace head\n",
    "num_classes = 8\n",
    "in_features = model.heads.head.in_features\n",
    "lora_model = LoRA.from_module(model, rank=5)\n",
    "lora_model.module.heads.head = nn.Linear(in_features, num_classes)\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in lora_model.named_parameters():\n",
    "    print(name, p.shape)#  if \"lora\" in name else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "# Initialize model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fac139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lora_pytorch import LoRA\n",
    "\n",
    "# Wrap your model with LoRA\n",
    "lora_model = LoRA.from_module(model, rank=5)\n",
    "\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac13e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weight Transforms\n",
    "weights = ResNet34_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0daa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x).squeeze()\n",
    "\n",
    "feature_extractor = FeatureExtractor(model)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5420a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641adb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet_V2_S_Weights, efficientnet_v2_s\n",
    "\n",
    "# Initialize model\n",
    "weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "model = efficientnet_v2_s(weights=weights)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Rimuovi il layer FC finale\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x).squeeze()\n",
    "\n",
    "# Usa il feature extractor\n",
    "feature_extractor = FeatureExtractor(model)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = random.choice(train_ds)\n",
    "img = img.unsqueeze(0)\n",
    "visualize_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = batch\n",
    "    features = feature_extractor(x)  # Shape: [batch_size, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RexNet import RexNet\n",
    "\n",
    "config = OmegaConf.load('config/config_rexnet.yaml')\n",
    "\n",
    "model = RexNet(config=config.model, num_classes=config.num_classes)\n",
    "\n",
    "# Carica direttamente dalla classe\n",
    "model = RexNet.load_from_checkpoint(\n",
    "    r'C:\\Users\\nicco\\OneDrive\\Documenti\\JurassicClass\\models\\cross_validation\\last_block_finetune\\RexNet\\fold_1\\best-epoch=11-val_loss=0.5470-val_acc=0.8599.ckpt',\n",
    "    config=config.model , # Devi passare gli argomenti non salvati    \n",
    "    strict=False  # Ignora i pesi mancanti/extra\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = batch\n",
    "    features = feature_extractor(x)  # Shape: [batch_size, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ce66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(img).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84625fa",
   "metadata": {},
   "source": [
    "#### Prova Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012268d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, idx_to_class, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = {\n",
    "        \"scores\": [],\n",
    "        \"predicted_names\": [],\n",
    "        \"real_names\": [],\n",
    "        \"correct\": [],\n",
    "        \"overall_accuracy\": None,\n",
    "    }\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x).softmax(1)\n",
    "            predicted_class_id = pred.argmax(1)\n",
    "\n",
    "            # Scores for predicted classes\n",
    "            idx = torch.arange(len(predicted_class_id))\n",
    "            score = pred[idx, predicted_class_id]\n",
    "\n",
    "            # Per-image correctness\n",
    "            correct = (predicted_class_id == y).float()\n",
    "\n",
    "            # Move to CPU for Python conversion\n",
    "            pred_cpu = predicted_class_id.cpu()\n",
    "            y_cpu = y.cpu()\n",
    "            score_cpu = score.cpu()\n",
    "            correct_cpu = correct.cpu()\n",
    "\n",
    "            # Append to results\n",
    "            results[\"scores\"].extend(score_cpu.tolist())\n",
    "            results[\"predicted_names\"].extend([idx_to_class[int(c)] for c in pred_cpu])\n",
    "            results[\"real_names\"].extend([idx_to_class[int(c)] for c in y_cpu])\n",
    "            results[\"correct\"].extend(correct_cpu.tolist())\n",
    "\n",
    "            # Update aggregated accuracy\n",
    "            total_correct += int(correct_cpu.sum().item())\n",
    "            total_samples += len(y_cpu)\n",
    "\n",
    "    # Final overall accuracy\n",
    "    results[\"overall_accuracy\"] = total_correct / total_samples\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_train(model, train_loader, val_loader, config, out_fold, in_fold, lora):\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        mode=\"min\",\n",
    "        patience=8)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"./models/cross_validation/{config.experiment_name}/{config.model.name}/fold_{out_fold}.{in_fold}\",\n",
    "        filename=\"best-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}\",  # Include both loss and accuracy\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    tb_logger = TensorBoardLogger(\n",
    "        save_dir=f\"./models/cross_validation/{config.experiment_name}/{config.model.name}\",  # parent dir\n",
    "        name=f\"tb_logs\",                                            # subfolder name\n",
    "        version=f\"fold_{out_fold}.{in_fold}\",                                     # unique run per fold\n",
    "        default_hp_metric=False                                      # optional: disable hp metric\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        default_root_dir=f\"./models/cross_validation/{config.experiment_name}/{model.model_name}/fold_{out_fold}.{in_fold}\",\n",
    "        logger=tb_logger,\n",
    "        callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "        enable_checkpointing=True,\n",
    "        log_every_n_steps=10,\n",
    "        limit_train_batches=0.25,\n",
    "        limit_val_batches=0.25,\n",
    "        max_epochs=4, # max_epochs=config.training.max_epochs,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader, )\n",
    "    \n",
    "    OmegaConf.save(config, f\"./models/cross_validation/{config.experiment_name}/{config.model.name}/fold_{out_fold}.{in_fold}/config.yaml\")\n",
    "    \n",
    "    return checkpoint_callback.best_model_path\n",
    "    \n",
    "\n",
    "def run_test(model_class, ckpt_path, test_loader, idx_to_class, config, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model_class.load_from_checkpoint(\n",
    "        ckpt_path,\n",
    "        config=config,\n",
    "        strict=False\n",
    "    )\n",
    "\n",
    "    results = evaluate_model(model, test_loader, idx_to_class, device=device)\n",
    "\n",
    "    ckpt_dir = os.path.dirname(ckpt_path)\n",
    "    pkl_path = os.path.join(ckpt_dir, \"test_results.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_outer_loop(train_ds, val_ds, config, network='EfficentRex', n_out=1, n_in=1, lora=False, rank=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kfold_out = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    acc_by_params = {}\n",
    "\n",
    "    experiment_name = \"lora\"\n",
    "\n",
    "    # outer loop\n",
    "    for out_fold, (train_idx_out, test_idx_out) in enumerate(kfold_out.split(train_ds, train_ds.targets)):\n",
    "        if out_fold > n_out:\n",
    "            break\n",
    "        print(f'============ out fold {out_fold} ============')\n",
    "        print(f'====================================')\n",
    "        train_subset_out = Subset(train_ds, train_idx_out)\n",
    "        test_subset_out = Subset(val_ds, test_idx_out)\n",
    "\n",
    "        # print(f\"\\tOut train subset size: {len(train_subset_out)},\\n\\tOut test subset size: {len(test_subset_out)}\")\n",
    "\n",
    "        outer_train_labels = [train_ds.targets[i] for i in train_idx_out]\n",
    "        kfold_in = StratifiedKFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "        # inner loop\n",
    "        for in_fold, (train_idx_in, val_idx_in) in enumerate(kfold_in.split(train_subset_out, outer_train_labels)):\n",
    "            if in_fold > n_in:\n",
    "                break\n",
    "            print(f'\\t============ in fold {in_fold} ============')\n",
    "\n",
    "            # map indxs of train_subset_out on the train_ds\n",
    "            train_indices = [train_idx_out[i] for i in train_idx_in]\n",
    "            val_indices   = [train_idx_out[i] for i in val_idx_in]\n",
    "\n",
    "            train_subset_in = Subset(train_ds, train_indices)\n",
    "            val_subset_in = Subset(val_ds, val_indices)\n",
    "\n",
    "            # print(f\"\\t\\tIn train subset size: {len(train_subset_in)},\\n\\t\\tIn validation subset size: {len(val_subset_in)}\")\n",
    "\n",
    "            base_cfg = OmegaConf.load(f'config/config_{network.lower()}.yaml')\n",
    "\n",
    "            train_loader = DataLoader(train_subset_in, batch_size=config.training.batch_size, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val_subset_in, batch_size=config.training.batch_size, shuffle=False, num_workers=0)\n",
    "            test_loader = DataLoader(test_subset_out, batch_size=config.training.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            if network.lower() == 'rexnet':\n",
    "                model_class = RexNet\n",
    "            elif network.lower() == 'efficentrex':\n",
    "                model_class = EfficentRex\n",
    "\n",
    "            param_grid = {\n",
    "                \"classifier_lr\": [5e-3, 3e-3],\n",
    "                \"factor\": [5, 10],\n",
    "            }\n",
    "            \n",
    "            for params in ParameterGrid(param_grid):\n",
    "                lr_cls = params[\"classifier_lr\"]\n",
    "                factor = params[\"factor\"]\n",
    "                \n",
    "                if lr_cls == 5e-3 and factor == 10:\n",
    "                    continue\n",
    "\n",
    "                cfg = copy.deepcopy(base_cfg)\n",
    "\n",
    "                layers_dict = cfg[\"model\"][\"layers_to_finetune\"]\n",
    "                first_layer_name = list(layers_dict.keys())[0]\n",
    "                layers_dict[first_layer_name][\"lr\"] = lr_cls\n",
    "\n",
    "                backbone_lr = lr_cls / factor\n",
    "                for layer_name in list(layers_dict.keys())[1:]:\n",
    "                    layers_dict[layer_name][\"lr\"] = backbone_lr\n",
    "\n",
    "                cfg[\"experiment_name\"] = experiment_name\n",
    "\n",
    "                cfg[\"model\"][\"name\"] = f\"{cfg['model']['name']}_firstlr{lr_cls:g}_fac{factor:g}\"\n",
    "            \n",
    "                model = model_class(config=cfg.model, num_classes=cfg.num_classes)\n",
    "                if lora:\n",
    "                    model = LoRA.from_module(model, rank=rank)\n",
    "                model.eval()\n",
    "                model.to(device)\n",
    "            \n",
    "                ckpt_path = run_train(model, train_loader, val_loader, cfg, out_fold, in_fold, lora)\n",
    "                \n",
    "                idx_to_class = {idx: class_ for class_, idx in train_ds.class_to_idx.items()}\n",
    "\n",
    "                results = run_test(\n",
    "                    model_class=model_class,\n",
    "                    ckpt_path=ckpt_path,\n",
    "                    test_loader=test_loader,\n",
    "                    idx_to_class=idx_to_class,\n",
    "                    config=cfg.model,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                acc = results[\"overall_accuracy\"]\n",
    "                \n",
    "                print(f\"\\t\\tAccuracy on test set: {acc}\")\n",
    "\n",
    "                key = (lr_cls, factor)\n",
    "                if key not in acc_by_params:\n",
    "                    acc_by_params[key] = []\n",
    "                acc_by_params[key].append(acc)\n",
    "\n",
    "\n",
    "    # cartella dove salvare il CSV (ad es. sperimentazione globale)\n",
    "    csv_dir = f\"./models/cross_validation/{experiment_name}\"\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(csv_dir, \"grid_search_results.csv\")\n",
    "\n",
    "    with open(csv_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"classifier_lr\", \"factor\", \"mean_test_accuracy\", \"std_test_accuracy\", \"n_runs\"])\n",
    "\n",
    "        for (lr_cls, factor), acc_list in acc_by_params.items():\n",
    "            mean_acc = float(np.mean(acc_list))\n",
    "            std_acc = float(np.std(acc_list))\n",
    "            n_runs = len(acc_list)\n",
    "            writer.writerow([lr_cls, factor, mean_acc, std_acc, n_runs])\n",
    "\n",
    "    print(f\"\\nSaved grid-search summary to: {csv_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "from omegaconf import OmegaConf\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torchvision.models import ResNet18_Weights, EfficientNet_V2_S_Weights\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ParameterGrid\n",
    "from torch.utils.data import Subset\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning import Trainer, Callback\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lora_pytorch import LoRA\n",
    "\n",
    "from scripts.utils import visualize_image, letterbox_to_square\n",
    "from scripts.EfficentRex import EfficentRex\n",
    "from scripts.RexNet import RexNet\n",
    "from scripts.ViTRex import ViTRex_FullFT\n",
    "from scripts.RexNet_FullFT import RexNet_FullFT\n",
    "from scripts.LoRaViTRex import LoRaViTRex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d857a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('config/config_loravitrex.yaml')\n",
    "model = LoRaViTRex(config=config.model, num_classes=config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('config/config_rexnet_fft.yaml')\n",
    "model = RexNet_FullFT(config=config.model, num_classes=config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cc116",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('config/config_vitrex_fft.yaml')\n",
    "model = ViTRex_FullFT(config=config.model, num_classes=config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b997ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    transforms.Lambda(lambda im: letterbox_to_square(im, size=config.size, fill=0)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    model.base_tfms,                                  # ToTensor + Normalize\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "    transforms.Lambda(lambda im: letterbox_to_square(im, size=config.size, fill=0)),\n",
    "    transforms.CenterCrop(224),\n",
    "    model.base_tfms,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = \"./dataset/dataset\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=f\"{root}/train\", transform=train_tfms)\n",
    "val_ds = datasets.ImageFolder(root=f\"{root}/train\", transform=val_tfms)  # Same data, different transforms\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "perm = torch.randperm(len(train_ds), generator=g).tolist()\n",
    "split = int(0.8 * len(train_ds))\n",
    "\n",
    "train_subset = Subset(train_ds, perm[:split])\n",
    "test_subset = Subset(val_ds, perm[split:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = random.choice(train_subset)\n",
    "img = img.unsqueeze(0)\n",
    "visualize_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=config.training.batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(test_subset, batch_size=config.training.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    mode=\"min\",\n",
    "    patience=12)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"./models/prova/{config.model.name}\",\n",
    "    filename=\"best-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}\",  # Include both loss and accuracy\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=f\"./models/prova/{config.model.name}\",  # parent dir\n",
    "    name=f\"tb_logs\",                                            # subfolder name\n",
    "    version=f\"lora_l\",                                     # unique run per fold\n",
    "    default_hp_metric=False                                      # optional: disable hp metric\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=f\"./models/prova/{model.model_name}\",\n",
    "    logger=tb_logger,\n",
    "    # limit_train_batches = 10,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=config.training.max_epochs,\n",
    "    enable_checkpointing=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d42c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=config.training.batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(test_subset, batch_size=config.training.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    mode=\"min\",\n",
    "    patience=12)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"./models/prova/{config.model.name}\",\n",
    "    filename=\"best-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}\",  # Include both loss and accuracy\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=f\"./models/prova/{config.model.name}\",  # parent dir\n",
    "    name=f\"tb_logs\",                                            # subfolder name\n",
    "    version=f\"long_warmup_2\",                                     # unique run per fold\n",
    "    default_hp_metric=False                                      # optional: disable hp metric\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=f\"./models/prova/{model.model_name}\",\n",
    "    logger=tb_logger,\n",
    "    # limit_train_batches = 10,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=config.training.max_epochs,\n",
    "    enable_checkpointing=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
