{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8028cc4",
   "metadata": {},
   "source": [
    "#### Download and deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f23e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def collect_url_and_download_images(specie: str, max_pages: int=50, out_folder: str='dataset'):\n",
    "    outdir = f\"{out_folder}/{specie}/raw\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    for query in [specie, f\"dinosaur {specie}\"]:\n",
    "        for i in range(max_pages):\n",
    "            results.extend(DDGS().images(\n",
    "                query=query,\n",
    "                region=\"us-en\",\n",
    "                safesearch=\"off\",\n",
    "                max_results=1000,\n",
    "                page=i))\n",
    "            \n",
    "    unique = {result[\"image\"] for result in results}\n",
    "    for idx, url in enumerate(unique, start=1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            r.raise_for_status()\n",
    "            ext = os.path.splitext(url.split(\"?\")[0])[1] or \".jpg\"\n",
    "            filename = os.path.join(outdir, f\"{idx:04d}{ext}\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {url} -> {e}\")\\\n",
    "                \n",
    "    return outdir\n",
    "            \n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def remove_duplicate_images(folder_path: str, similarity_threshold: int = 4):\n",
    "    \"\"\"\n",
    "    Remove duplicate/similar images using perceptual hashing.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing images\n",
    "        similarity_threshold: Lower = more strict (0=identical, 5=default, 10+=lenient)\n",
    "    \"\"\"\n",
    "    # Get all image files\n",
    "    image_files = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "    image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'))]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found in folder\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Checking {len(image_files)} images for duplicates...\")\n",
    "    \n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                \n",
    "                img_hash = imagehash.phash(img)\n",
    "                \n",
    "                # Check if similar image already exists\n",
    "                for existing_hash in hashes:\n",
    "                    if abs(img_hash - existing_hash) <= similarity_threshold:\n",
    "                        duplicates.append(img_path)\n",
    "                        print(f\"Duplicate: {os.path.basename(img_path)}\")\n",
    "                        break\n",
    "                else:\n",
    "                    hashes[img_hash] = img_path\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(img_path)}: {e}\")\n",
    "            print(f\"Deleting {os.path.basename(img_path)}\")\n",
    "            os.remove(img_path)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for duplicate in duplicates:\n",
    "        os.remove(duplicate)\n",
    "    \n",
    "    print(f\"Removed {len(duplicates)} duplicates. {len(image_files) - len(duplicates)} unique images remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995c414",
   "metadata": {},
   "source": [
    "### Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49a184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision ftfy regex tqdm pillow\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import os, shutil, glob\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_image(img_path, model, preprocess, device, \n",
    "                TXT_IS_DINO, TXT_NOT_DINO, TXT_REAL, TXT_NONREAL,\n",
    "                is_dino_prompts, not_dino_prompts, realistic_prompts, non_realistic_prompts,\n",
    "                verbose=False):\n",
    "    try:\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    img_feat = model.encode_image(image)\n",
    "    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def logits_for(prompts, txt_feats):\n",
    "        # cosine sims\n",
    "        sims = (img_feat @ txt_feats.T)  # [1, num_prompts]\n",
    "        # apply CLIP temperature (sharpens distribution)\n",
    "        scale = model.logit_scale.exp()\n",
    "        return sims * scale\n",
    "\n",
    "    # Get logits for each category\n",
    "    L_is   = logits_for(is_dino_prompts, TXT_IS_DINO)\n",
    "    L_not  = logits_for(not_dino_prompts, TXT_NOT_DINO)\n",
    "    L_real = logits_for(realistic_prompts, TXT_REAL)\n",
    "    L_non  = logits_for(non_realistic_prompts, TXT_NONREAL)\n",
    "\n",
    "    # 1) Dinosaur classification: max positive vs max negative\n",
    "    max_is_dino = L_is.max().item()\n",
    "    max_not_dino = L_not.max().item()\n",
    "    \n",
    "    # Softmax between the two max logits\n",
    "    dino_logits = torch.tensor([max_is_dino, max_not_dino])\n",
    "    dino_probs = torch.softmax(dino_logits, dim=0)\n",
    "    p_is_dino = dino_probs[0].item()\n",
    "    p_not_dino = dino_probs[1].item()\n",
    "\n",
    "    # 2) Realism classification: max positive vs max negative\n",
    "    max_real = L_real.max().item()\n",
    "    max_non_real = L_non.max().item()\n",
    "    \n",
    "    # Softmax between the two max logits\n",
    "    real_logits = torch.tensor([max_real, max_non_real])\n",
    "    real_probs = torch.softmax(real_logits, dim=0)\n",
    "    p_real = real_probs[0].item()\n",
    "    p_non_real = real_probs[1].item()\n",
    "\n",
    "    # margins in [âˆ’1, 1]; 0 means tie, >0 favors positives\n",
    "    is_dino_margin = p_is_dino - p_not_dino\n",
    "    realism_margin = p_real - p_non_real\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ‘‰ {os.path.basename(img_path)}\")\n",
    "        print(\"  [DINO LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(is_dino_prompts, L_is.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_is.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [NOT_DINO LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(not_dino_prompts, L_not.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_not.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [REALISTIC LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(realistic_prompts, L_real.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_real.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(\"  [NON_REALISTIC LOGITS]\")\n",
    "        for i, (s, logit) in enumerate(zip(non_realistic_prompts, L_non.squeeze(0).tolist())):\n",
    "            mark = \"â˜…\" if i == L_non.argmax().item() else \" \"\n",
    "            print(f\"    {mark} {s:<50} {logit:.3f}\")\n",
    "        print(f\"  Max dino: {max_is_dino:.3f} vs Max not-dino: {max_not_dino:.3f}\")\n",
    "        print(f\"  Max real: {max_real:.3f} vs Max non-real: {max_non_real:.3f}\")\n",
    "        print(f\"  p_is_dino={p_is_dino:.3f}  p_real={p_real:.3f}\")\n",
    "        print(f\"  is_dino_margin={is_dino_margin:.3f}  realism_margin={realism_margin:.3f}\")\n",
    "\n",
    "    return is_dino_margin, realism_margin\n",
    "\n",
    "def filter_folder(in_dir, out_good, out_bad, specie,\n",
    "                  dino_thr=0.20, realism_thr=0.15, verbose=False):\n",
    "    \n",
    "    os.makedirs(out_good, exist_ok=True)\n",
    "    os.makedirs(out_bad, exist_ok=True)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    # ---------- PROMPTS ----------\n",
    "    # 1) \"Ãˆ un dinosauro?\"\n",
    "    is_dino_prompts = [\n",
    "        \"a realistic illustration of a dinosaur\",\n",
    "        \"a realistic toy dinosaur figure\",\n",
    "        \"a photo of a dinosaur\",\n",
    "        \"a paleoart illustration of a dinosaur\",\n",
    "        f\"a realistic illustration of a {specie}\",\n",
    "        f\"a realistic toy {specie} figure\",\n",
    "        f\"a photo of a {specie}\",\n",
    "        f\"a paleoart illustration of a {specie}\",\n",
    "    ]\n",
    "    not_dino_prompts = [\n",
    "        \"a photo of a modern animal\",\n",
    "        \"a person or human\",\n",
    "        \"a landscape without animals\",\n",
    "        \"a vehicle or building\",\n",
    "        \"a toy\",\n",
    "        \"a cloth\",\n",
    "        \"an abstract image\",\n",
    "        \"a geometric figure\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 2) Realistico vs Cartoon/Toy/Fossile\n",
    "    realistic_prompts = [\n",
    "        \"a realistic illustration of a dinosaur\",\n",
    "        \"a dinosaur fossil skeleton\",\n",
    "        \"a realistic toy dinosaur figure\",\n",
    "        \"a high-quality render of a dinosaur\",\n",
    "        \n",
    "        f\"a realistic illustration of a {specie}\",\n",
    "        f\"a {specie} fossil skeleton\",\n",
    "        f\"a realistic toy {specie} figure\",\n",
    "        f\"a high-quality render of a {specie}\",\n",
    "    ]\n",
    "    non_realistic_prompts = [\n",
    "        \"a cartoon dinosaur for kids\",\n",
    "        \"a peluche toy dinosaur figure\",\n",
    "        \"a pixel art dinosaur\",\n",
    "        \"a simple line drawing of a dinosaur\",\n",
    "        \"a non realistic drawing of a dinosaur\"\n",
    "    ]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text_batch(prompts):\n",
    "        tokens = clip.tokenize(prompts).to(device)\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    TXT_IS_DINO = encode_text_batch(is_dino_prompts)\n",
    "    TXT_NOT_DINO = encode_text_batch(not_dino_prompts)\n",
    "    TXT_REAL = encode_text_batch(realistic_prompts)\n",
    "    TXT_NONREAL = encode_text_batch(non_realistic_prompts)\n",
    "\n",
    "    imgs = []\n",
    "    for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.bmp\"):\n",
    "        imgs += glob.glob(os.path.join(in_dir, ext))\n",
    "    kept = 0; rejected = 0\n",
    "\n",
    "    for p in tqdm(imgs, desc=f\"Filtering {os.path.basename(in_dir)}\"):\n",
    "        res = score_image(p, model, preprocess, device, \n",
    "                        TXT_IS_DINO, TXT_NOT_DINO, TXT_REAL, TXT_NONREAL,\n",
    "                        is_dino_prompts, not_dino_prompts, realistic_prompts, non_realistic_prompts, \n",
    "                        verbose=verbose)\n",
    "        if res is None:\n",
    "            rejected += 1\n",
    "            shutil.copy(p, os.path.join(out_bad, os.path.basename(p)))\n",
    "            continue\n",
    "        is_dino_m, realism_m = res\n",
    "\n",
    "        if (is_dino_m >= dino_thr) and (realism_m >= realism_thr):\n",
    "            kept += 1\n",
    "            shutil.copy(p, os.path.join(out_good, os.path.basename(p)))\n",
    "        else:\n",
    "            rejected += 1\n",
    "            shutil.copy(p, os.path.join(out_bad, os.path.basename(p)))\n",
    "\n",
    "    return kept, rejected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23877a8e",
   "metadata": {},
   "source": [
    "# pipeline for the 15 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985388f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\n",
    "    \"Ankylosaurus\",\n",
    "    \"Brachiosaurus\",\n",
    "    \"Compsognathus\",\n",
    "    \"Corythosaurus\",\n",
    "    \"Dilophosaurus\",\n",
    "    \"Dimorphodon\",\n",
    "    \"Gallimimus\",\n",
    "    \"Microceratus\",\n",
    "    \"Pachycephalosaurus\",\n",
    "    \"Parasaurolophus\",\n",
    "    \"Spinosaurus\",\n",
    "    \"Stegosaurus\",\n",
    "    \"Triceratops\",\n",
    "    \"Tyrannosaurus\",\n",
    "    \"Velociraptor\"\n",
    "]\n",
    "species = [\"Tyrannosaurus\"]\n",
    "for specie in species:\n",
    "    print(f\"processing dinosaur {specie}\")\n",
    "    specie_raw_dir = collect_url_and_download_images(specie=specie)\n",
    "    remove_duplicate_images(folder_path=specie_raw_dir)\n",
    "    print(f\"filtering dinosaur {specie}\")\n",
    "    kept, rej = filter_folder(\n",
    "        specie_raw_dir,\n",
    "        out_good=os.path.join(specie_raw_dir, \"clean\"),\n",
    "        out_bad=os.path.join(specie_raw_dir,\"rejected\"),\n",
    "        specie=specie,\n",
    "        dino_thr=0.20,\n",
    "        realism_thr=0.20)\n",
    "    print(f\"Specie: {specie}. Kept {(kept)}, ejected {(rej)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24c27ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Ankylosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 597/597 [00:00<00:00, 665.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Brachiosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 725.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Compsognathus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:04<00:00, 102.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Corythosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 390/390 [00:05<00:00, 76.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Dilophosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:07<00:00, 70.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Dimorphodon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:05<00:00, 70.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Gallimimus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 494/494 [00:06<00:00, 72.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Microceratus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 425/425 [00:06<00:00, 68.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Pachycephalosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:06<00:00, 70.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Parasaurolophus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 504/504 [00:07<00:00, 67.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Spinosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545/545 [00:08<00:00, 67.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Stegosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 647/647 [00:09<00:00, 69.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Triceratops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650/650 [00:09<00:00, 68.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Tyrannosaurus_Rex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392/392 [00:05<00:00, 66.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dino: Velociraptor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:07<00:00, 65.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for dinosaurs in species:\n",
    "    print(f'dino: {dinosaurs}')\n",
    "    folder = f'./dataset/{dinosaurs}/raw/clean'\n",
    "    out_folder = f'dataset/to_phone/{dinosaurs}'\n",
    "    #os.makedirs(out_folder)\n",
    "    for file_name in tqdm(os.listdir(folder)):\n",
    "        # construct full file path\n",
    "        source = os.path.join(folder, file_name)\n",
    "        destination = os.path.join(out_folder, file_name)\n",
    "        # copy only files\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy(source, destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a1d8c",
   "metadata": {},
   "source": [
    "# After manual curation of the dataset - Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39ef604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microceratus\n",
      "Pachycephalosaurus\n",
      "Parasaurolophus\n",
      "Spinosaurus\n",
      "Stegosaurus\n",
      "Triceratops\n",
      "Tyrannosaurus\n",
      "Velociraptor\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "curated_path = './dataset/hand_curated_datasets'\n",
    "final_dataset = './dataset/dataset'\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "for species_sub_folder in os.listdir(curated_path):\n",
    "    print(species_sub_folder)\n",
    "    species_images = os.listdir(os.path.join(curated_path,species_sub_folder))\n",
    "    species_images = [image for image in species_images if not image.startswith('.')] \n",
    "    indices = random.sample(range(len(species_images)), int(0.15*len(species_images)))\n",
    "    os.makedirs(os.path.join(final_dataset, 'train', species_sub_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(final_dataset, 'test', species_sub_folder), exist_ok=True)\n",
    "    \n",
    "    for idx, image in enumerate(species_images):\n",
    "        path_to_image = os.path.join(curated_path, species_sub_folder, image)\n",
    "        split = 'test' if idx in indices else 'train'\n",
    "        dst_path = os.path.join(final_dataset, split, species_sub_folder, image)\n",
    "        try:\n",
    "            img = Image.open(path_to_image)\n",
    "            png_filename = os.path.splitext(image)[0] + '.png'\n",
    "            dst_path = os.path.join(final_dataset, split, species_sub_folder, png_filename)\n",
    "            img.save(dst_path, 'PNG')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image}: {e}\")\n",
    "         \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95011b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),       # force 3 channels\n",
    "    transforms.RandomResizedCrop(224, antialias=True),       # preserves ratio, random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.Resize(256, antialias=True),                  # resize short side to 256\n",
    "    transforms.CenterCrop(224),                              # now 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "root = \"./dataset/dataset\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=f\"{root}/train\", transform=train_tfms)\n",
    "test_ds  = datasets.ImageFolder(root=f\"{root}/test\",  transform=test_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
